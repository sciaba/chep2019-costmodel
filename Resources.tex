\section{Resource estimation}
An important step in the process of calculating the cost of computing
is to estimate the amount of resources (CPU, disk, tape and network)
needed to fulfill the phyics programme of the experiment. Having a
sufficiently complex and flexible model is a prerequisite to produce
reliable estimates, at some point it was proposed to develop a common
framework that could be used by all LHC experiments. Although such
framework naver came to be, the experiments are now in a much better
situation, having modular software-based frameworks instead of the
unwieldy spreadsheets that were used for many years. For example,
ATLAS and CMS have now frameworks very much comparable
in terms of functionality and parameters.

Recent work in CMS~\cite{cmsres} focused in adding for the first time
estimates for the required tape I/O bandwidth at HL-LHC and for the
network capacity; there are still significant uncertainties though,
like the future role of GPUs and accelerators, which is impossible to
quantify at this point in time.

Given the maturity reached by this estimation process, it is
reasonable to assume that the cost model working group will not need
to play a role any longer, but for facilitating exchange of
information among experiments and identifying possible gaps that would
need to be addressed.
