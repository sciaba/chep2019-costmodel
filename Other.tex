\section{Other improvements}
A very promising work has started, to study the impact of storage
caches on the cost of computing for smaller sites. The idea behind it
is the consolidation of managed storage at a few large sites forming
``data lakes'', and its replacement with storage caches
elsewhere. This would bring several advantages: lessen the need to
create long-lived replicas of datasets (simpler data management),
reduce the amount of ``cold data'' (less storage used), replace
complex storage services with simpler ones (lighter site operations)
and allow for less redundant and expensive storage configurations
(less cost for TB). This work is using actual file access data from
ATLAS and CMS and is described in detail elsewhere in these
proceedings~\cite{cache}.

\section{Cost effectiveness of HPC resources}
An aspect of the computing evolution that has not yet been completely
understood is the scenario, particularly likely in certain regions,
where computing resources at a national level are increasingly
provided by supercomputing (HPC) centers at the detriment of more
``traditional'' computing centers, like those operating as Tier-1s and
Tier-2s in WLCG. Using HPC centers for the LHC experiments workloads
presents considerable challenges, among which:
\begin{itemize}
\item hardware heterogeneity: non-x86 CPUs, GPUs, accelerators
\item different authorization/authentication systems, network
  restrictions, time-limited resource allocations, etc.
\item quantification of the usable resources with respect to each
  relevant workload
\end{itemize}

The last point in particular was the subject of a preliminary proposal
by the cost model and the HEPiX benchmarking working groups, to define
a practical procedure to map the capacity of an HPC resource to a WLCG
``pledge'', briefly described here.

The first step consists of running at a small scale an eligible
workload $A_1$ for a certain time on a certain number of cores and
accelerators (not shared with other workloads) and measure the amount
of CPU time $T_{CPU}$ (cores$\cdot$s) and accelerator time $T_{acc}$
(accelerators$\cdot$s) needed to process one event. Subsequently, the
same workload is run on a ``traditional'' system and the amount of
HS06$\cdot$s~\cite{hs06} to process one event is measured. By equating
the computing resource usages in the two cases, one can translate a
given WLCG pledge to a certain amount of processing time on a given
HPC resource. In reality, one will have to take into account also
bottleneck effects, for example when the accelerator is underutilized
and the CPU is the limiting factor. It is important to know that this
equivalence is in principle different for each workload, as each one
may use differently the resources, and different for each HPC system,
potentially leading to a very large number of combinations; in
practice, we can expect only a handful of systems to be used on the
few workloads that can use them most efficiently.

Another important metric to be defined would be a measure of how
``well'' we are exploiting an HPC resource, that we could call
\emph{realized potential}. A possibility is to use as reference the
$R_{max}$ FLOPS measured by LINPACK to express the
maximum achievable performance of the HPC system; integrating it over
the length of the time allocation, it produces a certain amount of
FLOP for the CPUs and the GPUs. While the workload is run, the CPU and
GPU utilization levels are measured (as a percent), multiplied by the
amounts of FLOP calculated above, and the total FLOP utilized is
divided by the total FLOP achievable. This ratio would represent how
much of the computing power of the HPC resource is actually used, and
it could be used to determine which workloads are best run on the
resource, and which resources are most cost-effective for the pledge
provider.

In reality, experience shows that running on an HPC resource usually
needs a considerable amount of preparation, which is not taken into
account in the metrics defined above. It is also clear that, at this
time, very limited use can be done (if any) of non-CPU resources for
production workloads; however, this is going to change, as more HEP
workloads become able to utilize GPUs.
